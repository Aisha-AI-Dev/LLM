{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class ArithmeticDataset(Dataset):\n",
    "    def __init__(self, max_length, num_samples):\n",
    "        self.max_length = max_length\n",
    "        self.num_samples = num_samples\n",
    "        self.data = self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def generate_number(self, length):\n",
    "        return random.randint(10**(length-1), 10**length - 1)\n",
    "\n",
    "    def generate_data(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(ArithmeticDataset):\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        samples_per_combination = self.num_samples // (self.max_length ** 2)\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            for j in range(1, self.max_length + 1):\n",
    "                for _ in range(samples_per_combination):\n",
    "                    num1 = self.generate_number(i)\n",
    "                    num2 = self.generate_number(j)\n",
    "                    result = num1 + num2\n",
    "                    data.append((f\"{num1}+{num2}=\", str(result)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationDataset(ArithmeticDataset):\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        samples_per_combination = self.num_samples // (self.max_length ** 2)\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            for j in range(1, self.max_length + 1):\n",
    "                for _ in range(samples_per_combination):\n",
    "                    num1 = self.generate_number(i)\n",
    "                    num2 = self.generate_number(j)\n",
    "                    result = num1 * num2\n",
    "                    data.append((f\"{num1}*{num2}=\", str(result)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortingDataset(ArithmeticDataset):\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        samples_per_combination = self.num_samples // (self.max_length ** 2)\n",
    "        for i in range(1, self.max_length + 1):  # number of integers\n",
    "            for j in range(1, self.max_length + 1):  # max digit length\n",
    "                for _ in range(samples_per_combination):\n",
    "                    numbers = [self.generate_number(random.randint(1, j)) for _ in range(i)]\n",
    "                    indices = list('abcdefghijklmnopqrstuvwxyz'[:i])\n",
    "                    input_str = ','.join([f\"{idx}:{num}\" for idx, num in zip(indices, numbers)])\n",
    "                    sorted_indices = [idx for _, idx in sorted(zip(numbers, indices))]\n",
    "                    output_str = ''.join(sorted_indices)\n",
    "                    data.append((input_str, output_str))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_class, max_length, train_samples, test_samples):\n",
    "    train_dataset = dataset_class(max_length, train_samples)\n",
    "    test_dataset = dataset_class(max_length, test_samples)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition sample: ('4+5=', '9')\n",
      "Multiplication sample: ('6*4=', '24')\n",
      "Sorting sample: ('a:3', 'a')\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "max_length = 20  # maximum length of operands\n",
    "train_samples = 200_000  # 20 million as mentioned in the paper\n",
    "test_samples = 1_000  # adjust as needed\n",
    "\n",
    "# Create datasets\n",
    "addition_train, addition_test = create_datasets(AdditionDataset, max_length, train_samples, test_samples)\n",
    "multiplication_train, multiplication_test = create_datasets(MultiplicationDataset, max_length, train_samples, test_samples)\n",
    "sorting_train, sorting_test = create_datasets(SortingDataset, max_length, train_samples, test_samples)\n",
    "\n",
    "# Print some samples\n",
    "print(\"Addition sample:\", addition_train[0])\n",
    "print(\"Multiplication sample:\", multiplication_train[0])\n",
    "print(\"Sorting sample:\", sorting_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Addition Samples:\n",
      "Input: 135748326+108631275=, Output: 244379601\n",
      "Input: 1354172+336685848355199866=, Output: 336685848356554038\n",
      "Input: 44338320546283+972243690165=, Output: 45310564236448\n",
      "Input: 75040161704634192095+9527451304538227395=, Output: 84567613009172419490\n",
      "Input: 9318+84623911195379659736=, Output: 84623911195379669054\n",
      "Input: 58739854+8279644088=, Output: 8338383942\n",
      "Input: 9395564888+77629=, Output: 9395642517\n",
      "Input: 520+2371=, Output: 2891\n",
      "Input: 5623061952901691460+3=, Output: 5623061952901691463\n",
      "Input: 243+4925787039520747=, Output: 4925787039520990\n",
      "\n",
      "Multiplication Samples:\n",
      "Input: 1314*4727797=, Output: 6212325258\n",
      "Input: 3228169008960561*26=, Output: 83932394232974586\n",
      "Input: 7035962375386837078*20466463929=, Output: 144001270161655858478286759462\n",
      "Input: 4830844194151411*4972082029=, Output: 24019353602639217538092919\n",
      "Input: 68464*476769075851088=, Output: 32641518009068888832\n",
      "Input: 3517314548080498*75912257251479930246=, Output: 267007286808259638068510650232942508\n",
      "Input: 9157701523120469911*24=, Output: 219784836554891277864\n",
      "Input: 9815449*39=, Output: 382802511\n",
      "Input: 222564*1319102320=, Output: 293584688748480\n",
      "Input: 5103973233470414*7566984585775=, Output: 38621686783878808047507760850\n",
      "\n",
      "Sorting Samples:\n",
      "Input: a:89850,b:1,c:210273, Output: bac\n",
      "Input: a:92422980069,b:955,c:166596852822693,d:33168598266,e:9643845828,f:7128107932,g:240138997812,h:861004747392283,i:5,j:649348680291496,k:899641678955276,l:210912946985052,m:7,n:3946970,o:202021969566434, Output: imbnfedagcoljhk\n",
      "Input: a:4,b:56203099, Output: ab\n",
      "Input: a:11,b:88,c:2,d:87,e:3,f:4,g:805,h:844,i:31,j:121,k:91,l:819,m:8,n:4,o:75, Output: cefnmaiodbkjglh\n",
      "Input: a:11, Output: a\n",
      "Input: a:9,b:9, Output: ab\n",
      "Input: a:25,b:2,c:27492444666,d:553,e:74497453366,f:248566,g:1697,h:50303842,i:341,j:90,k:913039,l:64421985863,m:3262,n:7,o:26425471,p:75906992651,q:973,r:24251515,s:2172,t:25242, Output: bnajidqgsmtfkrohclep\n",
      "Input: a:5,b:4, Output: ba\n",
      "Input: a:9614,b:413619,c:2103866,d:525,e:35,f:497705,g:805690,h:34135,i:66437,j:5,k:56498,l:48097670,m:9982,n:626,o:15513,p:47, Output: jepdnamohkibfgcl\n",
      "Input: a:821,b:51872,c:9,d:72,e:10414,f:98082,g:41,h:984,i:30,j:2,k:9898,l:13, Output: jcligdahkebf\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def print_samples(dataset, name, num_samples=10):\n",
    "    print(f\"\\n{name} Samples:\")\n",
    "    for _ in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        sample = dataset[idx]\n",
    "        print(f\"Input: {sample[0]}, Output: {sample[1]}\")\n",
    "\n",
    "# Sample from Addition dataset\n",
    "print_samples(addition_train, \"Addition\")\n",
    "\n",
    "# Sample from Multiplication dataset\n",
    "print_samples(multiplication_train, \"Multiplication\")\n",
    "\n",
    "# Sample from Sorting dataset\n",
    "print_samples(sorting_train, \"Sorting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Transformer for Arithmetic Tasks\n",
    "\n",
    "This code implements a small transformer model designed to learn basic arithmetic operations, inspired by the Abacus Embeddings paper. The model architecture is as follows:\n",
    "\n",
    "## Model Architecture\n",
    "- Embedding layer: Custom Abacus Embedding\n",
    "- Transformer layers: 2\n",
    "- Attention heads per layer: 2\n",
    "- Embedding dimension: 64\n",
    "- Feed-forward dimension: 128\n",
    "- Maximum sequence length: 20\n",
    "\n",
    "## Key Components\n",
    "1. **AbacusEmbedding**: A custom embedding layer that combines token embeddings with positional information.\n",
    "2. **SmallTransformer**: The main model class, incorporating the Abacus Embedding and transformer layers.\n",
    "3. **Training Loop**: Includes both training and evaluation phases, tracking loss and accuracy.\n",
    "\n",
    "## Training Details\n",
    "- Dataset: Addition task (can be extended to multiplication and sorting)\n",
    "- Batch size: 32\n",
    "- Number of epochs: 10\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Loss function: Cross Entropy Loss (ignoring padding tokens)\n",
    "\n",
    "This setup allows for quick experimentation and debugging on a CPU. Once the basic functionality is verified, the model size and dataset can be scaled up to match the specifications in the Abacus Embeddings paper.\n",
    "\n",
    "Let's calculate the number of parameters for this model configuration. We'll break it down by component:\n",
    "\n",
    "1. Embedding Layer:\n",
    "   - Token Embedding: vocab_size * embed_size = 14 * 64 = 896\n",
    "   - Positional Embedding: max_length * embed_size = 20 * 64 = 1,280\n",
    "\n",
    "2. Transformer Layers (for each layer):\n",
    "   - Self-Attention:\n",
    "     * Query, Key, Value matrices: 3 * (embed_size * embed_size) = 3 * (64 * 64) = 12,288\n",
    "     * Output projection: embed_size * embed_size = 64 * 64 = 4,096\n",
    "   - Feed-forward network:\n",
    "     * First linear layer: embed_size * ff_dim = 64 * 128 = 8,192\n",
    "     * Second linear layer: ff_dim * embed_size = 128 * 64 = 8,192\n",
    "   - Layer Norm (2 per layer): 2 * 2 * embed_size = 2 * 2 * 64 = 256\n",
    "\n",
    "   Total per layer: 12,288 + 4,096 + 8,192 + 8,192 + 256 = 33,024\n",
    "\n",
    "3. Output Layer:\n",
    "   - Linear projection: embed_size * vocab_size = 64 * 14 = 896\n",
    "\n",
    "Now, let's sum it up:\n",
    "- Embedding Layer: 896 + 1,280 = 2,176\n",
    "- Transformer Layers: 33,024 * 2 = 66,048\n",
    "- Output Layer: 896\n",
    "\n",
    "Total parameters: 2,176 + 66,048 + 896 = 69,120\n",
    "\n",
    "So, this small transformer model would have approximately 69,120 parameters.\n",
    "\n",
    "This is a very small model, which is perfect for initial experiments and debugging on a CPU. It's about 3 orders of magnitude smaller than the models described in the Abacus Embeddings paper (which mentions models with ~12 million parameters), allowing for quick iterations and tests of the basic architecture and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, max_length, num_samples):\n",
    "        # Initialize the dataset with maximum length of numbers and total samples\n",
    "        self.max_length = max_length\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Define the vocabulary for tokenization\n",
    "        # 0-9 for digits, 10 for '+', 11 for '=', 12 for padding, 13 for end of sequence\n",
    "        self.vocab = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, \n",
    "                      '+': 10, '=': 11, '<PAD>': 12, '<EOS>': 13}\n",
    "        # Create an inverse vocabulary for decoding\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        # Generate the dataset\n",
    "        self.data = self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a specific item from the dataset\n",
    "        return self.data[idx]\n",
    "\n",
    "    def generate_number(self, length):\n",
    "        # Generate a random number of specified length\n",
    "        return random.randint(10**(length-1), 10**length - 1)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        # Convert a string to a list of token IDs\n",
    "        return [self.vocab[c] for c in s if c in self.vocab]\n",
    "\n",
    "    def pad_sequence(self, seq, max_length):\n",
    "        # Pad a sequence with <PAD> tokens to reach the specified length\n",
    "        return seq + [self.vocab['<PAD>']] * (max_length - len(seq))\n",
    "\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        # Calculate samples per length combination to achieve desired total samples\n",
    "        samples_per_combination = max(1, self.num_samples // (self.max_length ** 2))\n",
    "        \n",
    "        # Generate addition problems for all possible length combinations\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            for j in range(1, self.max_length + 1):\n",
    "                for _ in range(samples_per_combination):\n",
    "                    # Generate two random numbers\n",
    "                    num1 = self.generate_number(i)\n",
    "                    num2 = self.generate_number(j)\n",
    "                    result = num1 + num2\n",
    "                    \n",
    "                    # Create the input string (reversed for right-to-left processing)\n",
    "                    input_str = f\"{num1:0{i}}+{num2:0{j}}=\"\n",
    "                    input_str = input_str[::-1]  # Reverse the string\n",
    "                    \n",
    "                    # Create the target string (reversed)\n",
    "                    target_str = f\"{result}\"[::-1]\n",
    "                    \n",
    "                    # Tokenize and pad both input and target\n",
    "                    input_tokens = self.tokenize(input_str)\n",
    "                    target_tokens = self.tokenize(target_str) + [self.vocab['<EOS>']]\n",
    "                    \n",
    "                    max_seq_length = self.max_length * 2 + 2  # Maximum possible sequence length\n",
    "                    input_padded = self.pad_sequence(input_tokens, max_seq_length)\n",
    "                    target_padded = self.pad_sequence(target_tokens, max_seq_length)\n",
    "                    \n",
    "                    # Convert to PyTorch tensors\n",
    "                    input_tensor = torch.tensor(input_padded, dtype=torch.long)\n",
    "                    target_tensor = torch.tensor(target_padded, dtype=torch.long)\n",
    "                    \n",
    "                    data.append((input_tensor, target_tensor))\n",
    "        \n",
    "        # Shuffle the data for randomness\n",
    "        random.shuffle(data)\n",
    "        return data\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        # Convert a tensor of token IDs back to a string, reversing and removing special tokens\n",
    "        return ''.join(self.inv_vocab[t.item()] for t in tensor if t.item() not in [self.vocab['<PAD>'], self.vocab['<EOS>']])[::-1]\n",
    "\n",
    "# Set parameters for the dataset\n",
    "max_length = 20  # maximum length of operands\n",
    "train_samples = 200_000  # Number of training samples\n",
    "test_samples = 1_000  # Number of test samples\n",
    "\n",
    "# Create training and test datasets\n",
    "addition_train = AdditionDataset(max_length, train_samples)\n",
    "addition_test = AdditionDataset(max_length, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition samples:\n",
      "Input: 393428+1=\n",
      "Target: 393429\n",
      "Equation: 393428+1= 393429\n",
      "\n",
      "Input: 90607628961844151+205165=\n",
      "Target: 90607628962049316\n",
      "Equation: 90607628961844151+205165= 90607628962049316\n",
      "\n",
      "Input: 959557+87=\n",
      "Target: 959644\n",
      "Equation: 959557+87= 959644\n",
      "\n",
      "Input: 32346322338+39666860=\n",
      "Target: 32385989198\n",
      "Equation: 32346322338+39666860= 32385989198\n",
      "\n",
      "Input: 950711420+26900238=\n",
      "Target: 977611658\n",
      "Equation: 950711420+26900238= 977611658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "print(\"Addition samples:\")\n",
    "for i in range(0,5):\n",
    "    input_tensor, target_tensor = addition_train[i]\n",
    "    input_str = addition_train.decode(input_tensor)\n",
    "    target_str = addition_train.decode(target_tensor)\n",
    "    print(f\"Input: {input_str}\")\n",
    "    print(f\"Target: {target_str}\")\n",
    "    print(f\"Equation: {input_str} {target_str}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbacusEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_length):\n",
    "        super().__init__()\n",
    "        # Create an embedding layer for the input tokens\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Create a separate embedding layer for positional encodings\n",
    "        self.pos_embed = nn.Embedding(max_length, embed_size)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get the sequence length of the input\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Generate position indices\n",
    "        pos = torch.arange(seq_length, device=x.device).unsqueeze(0)\n",
    "        \n",
    "        # Truncate positions to max_length\n",
    "        # This ensures that positions beyond max_length use the same embedding\n",
    "        pos = torch.clamp(pos, max=self.max_length - 1)\n",
    "        \n",
    "        # Get the token embeddings\n",
    "        embedded = self.embed(x)\n",
    "        \n",
    "        # Get the positional embeddings\n",
    "        positional = self.pos_embed(pos)\n",
    "        \n",
    "        # Combine token embeddings and positional embeddings\n",
    "        return embedded + positional[:, :seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, ff_dim, num_layers, max_length):\n",
    "        super().__init__()\n",
    "        # Initialize the custom Abacus Embedding layer\n",
    "        self.embedding = AbacusEmbedding(vocab_size, embed_size, max_length)\n",
    "        \n",
    "        # Create a single Transformer encoder layer\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Create the full Transformer encoder by stacking multiple layers\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final linear layer to project to vocabulary size\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # Apply Abacus Embedding\n",
    "            x = self.embedding(x)\n",
    "            \n",
    "            # Pass through the Transformer encoder\n",
    "            x = self.transformer(x)\n",
    "            \n",
    "            # Project to vocabulary size\n",
    "            return self.fc_out(x)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in SmallTransformer forward pass: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a progress bar for each epoch\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in progress_bar:\n",
    "            try:\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy for this batch\n",
    "                _, predicted = outputs.max(dim=-1)\n",
    "                non_pad_mask = targets.ne(addition_train.vocab['<PAD>'])\n",
    "                correct_predictions += (predicted[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "                total_predictions += non_pad_mask.sum().item()\n",
    "\n",
    "                # Update progress bar with current loss and accuracy\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'acc': f\"{correct_predictions/total_predictions:.4f}\"\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                # Error handling and debugging information\n",
    "                print(f\"\\nError in batch {batch_idx}\")\n",
    "                print(f\"Input shape: {inputs.shape}, max value: {inputs.max().item()}, min value: {inputs.min().item()}\")\n",
    "                print(f\"Target shape: {targets.shape}, max value: {targets.max().item()}, min value: {targets.min().item()}\")\n",
    "                print(f\"Output shape: {outputs.shape}\")\n",
    "                raise e\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Evaluation on test set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(dim=-1)\n",
    "                non_pad_mask = targets.ne(addition_train.vocab['<PAD>'])\n",
    "                total += non_pad_mask.sum().item()\n",
    "                correct += (predicted[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "                \n",
    "                # Calculate test loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        # Calculate test accuracy and average test loss\n",
    "        test_accuracy = correct / total\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'New best model saved with accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "        print('-' * 60)\n",
    "\n",
    "    print(f'Training completed. Best test accuracy: {best_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [03:46<00:00, 27.65it/s, loss=1.3243, acc=0.3827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 - Time: 226.04s\n",
      "Train Loss: 1.6537, Train Accuracy: 0.3827\n",
      "Test Loss: 1.4574, Test Accuracy: 0.4344\n",
      "New best model saved with accuracy: 0.4344\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [03:54<00:00, 26.67it/s, loss=1.6127, acc=0.4225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 - Time: 234.32s\n",
      "Train Loss: 1.5003, Train Accuracy: 0.4225\n",
      "Test Loss: 1.4118, Test Accuracy: 0.4449\n",
      "New best model saved with accuracy: 0.4449\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [03:45<00:00, 27.69it/s, loss=1.3877, acc=0.4366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 - Time: 225.70s\n",
      "Train Loss: 1.4610, Train Accuracy: 0.4366\n",
      "Test Loss: 1.3644, Test Accuracy: 0.4725\n",
      "New best model saved with accuracy: 0.4725\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [03:49<00:00, 27.28it/s, loss=1.4293, acc=0.4684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 - Time: 229.11s\n",
      "Train Loss: 1.3960, Train Accuracy: 0.4684\n",
      "Test Loss: 1.2328, Test Accuracy: 0.5350\n",
      "New best model saved with accuracy: 0.5350\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [18:04<00:00,  5.76it/s, loss=1.2539, acc=0.5084]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 - Time: 1084.67s\n",
      "Train Loss: 1.3149, Train Accuracy: 0.5084\n",
      "Test Loss: 1.1163, Test Accuracy: 0.5858\n",
      "New best model saved with accuracy: 0.5858\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [03:58<00:00, 26.24it/s, loss=1.1787, acc=0.5408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 - Time: 238.23s\n",
      "Train Loss: 1.2441, Train Accuracy: 0.5408\n",
      "Test Loss: 1.0412, Test Accuracy: 0.6149\n",
      "New best model saved with accuracy: 0.6149\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [03:26<00:00, 30.26it/s, loss=1.3563, acc=0.5738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 - Time: 206.56s\n",
      "Train Loss: 1.1618, Train Accuracy: 0.5738\n",
      "Test Loss: 0.9037, Test Accuracy: 0.6607\n",
      "New best model saved with accuracy: 0.6607\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [03:28<00:00, 29.99it/s, loss=1.3578, acc=0.5975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 - Time: 208.38s\n",
      "Train Loss: 1.0946, Train Accuracy: 0.5975\n",
      "Test Loss: 0.8801, Test Accuracy: 0.6699\n",
      "New best model saved with accuracy: 0.6699\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [03:45<00:00, 27.66it/s, loss=0.9276, acc=0.6073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 - Time: 225.94s\n",
      "Train Loss: 1.0679, Train Accuracy: 0.6073\n",
      "Test Loss: 0.8410, Test Accuracy: 0.6808\n",
      "New best model saved with accuracy: 0.6808\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [03:00<00:00, 34.56it/s, loss=0.9931, acc=0.6154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 - Time: 180.84s\n",
      "Train Loss: 1.0465, Train Accuracy: 0.6154\n",
      "Test Loss: 0.8201, Test Accuracy: 0.6943\n",
      "New best model saved with accuracy: 0.6943\n",
      "------------------------------------------------------------\n",
      "Training completed. Best test accuracy: 0.6943\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "vocab_size = 14  # 0-9 digits <PAD>, <EOS>, +, =,\n",
    "embed_size = 64\n",
    "num_heads = 2\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "max_length = 20\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(addition_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(addition_test, batch_size=batch_size)\n",
    "\n",
    "max_seq_length = max_length * 2 + 2  # This should be 42 based on your current setup\n",
    "model = SmallTransformer(vocab_size, embed_size, num_heads, ff_dim, num_layers, max_seq_length)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_size-2)  # Assuming <PAD> is the second to last token\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After training is complete\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab_size,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_size\u001b[39m\u001b[38;5;124m'\u001b[39m: embed_size,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_heads\u001b[39m\u001b[38;5;124m'\u001b[39m: num_heads,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: ff_dim,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: num_layers,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m'\u001b[39m: max_seq_length\n\u001b[1;32m     11\u001b[0m }, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_addition_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_size': embed_size,\n",
    "    'num_heads': num_heads,\n",
    "    'ff_dim': ff_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'max_seq_length': max_seq_length\n",
    "}, 'trained_addition_model.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "checkpoint = torch.load('trained_addition_model.pth')\n",
    "\n",
    "# Recreate the model architecture\n",
    "loaded_model = SmallTransformer(\n",
    "    checkpoint['vocab_size'],\n",
    "    checkpoint['embed_size'],\n",
    "    checkpoint['num_heads'],\n",
    "    checkpoint['ff_dim'],\n",
    "    checkpoint['num_layers'],\n",
    "    checkpoint['max_seq_length']\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Function to preprocess input for the model\n",
    "def preprocess_input(input_str, max_length):\n",
    "    # Reverse the input string\n",
    "    input_str = input_str[::-1]\n",
    "    # Tokenize\n",
    "    tokens = [addition_train.vocab[c] for c in input_str if c in addition_train.vocab]\n",
    "    # Pad\n",
    "    padded = tokens + [addition_train.vocab['<PAD>']] * (max_length - len(tokens))\n",
    "    return torch.tensor(padded).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Function to decode model output\n",
    "def decode_output(output_tensor):\n",
    "    _, predicted = output_tensor.max(2)\n",
    "    decoded = ''.join([addition_train.inv_vocab[t.item()] for t in predicted[0] if t.item() not in [addition_train.vocab['<PAD>'], addition_train.vocab['<EOS>']]])\n",
    "    return decoded[::-1]  # Reverse the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 + 4 = 011004\n",
      "Correct result: 1004\n",
      "Model's prediction is incorrect\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a single addition problem\n",
    "def test_addition(num1, num2):\n",
    "    input_str = f\"{num1}+{num2}=\"\n",
    "    input_tensor = preprocess_input(input_str, checkpoint['max_seq_length'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = loaded_model(input_tensor)\n",
    "    \n",
    "    result = decode_output(output)\n",
    "    print(f\"{num1} + {num2} = {result}\")\n",
    "    print(f\"Correct result: {num1 + num2}\")\n",
    "    print(f\"Model's prediction is {'correct' if int(result) == num1 + num2 else 'incorrect'}\")\n",
    "\n",
    "# Test on some examples\n",
    "#test_addition(123, 456)\n",
    "#test_addition(7890, 1234)\n",
    "test_addition(1000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_test_set(num_samples, max_digits):\n",
    "    test_set = []\n",
    "    for _ in range(num_samples):\n",
    "        num1 = random.randint(1, 10**max_digits - 1)\n",
    "        num2 = random.randint(1, 10**max_digits - 1)\n",
    "        result = num1 + num2\n",
    "        test_set.append((num1, num2, result))\n",
    "    return test_set\n",
    "\n",
    "# Generate a test set\n",
    "num_test_samples = 1000\n",
    "max_test_digits = 20  # Maximum number of digits in each operand\n",
    "test_set = generate_test_set(num_test_samples, max_test_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy: 0.6881\n",
      "Test Data Accuracy: 0.6943\n",
      "\n",
      "Accuracy comparison:\n",
      "Training Data: 0.6881\n",
      "Test Data:     0.6943\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_dataset(model, dataloader, dataset_name=\"Dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(2)\n",
    "            \n",
    "            # Create a mask for non-padding tokens\n",
    "            non_pad_mask = targets.ne(addition_train.vocab['<PAD>'])\n",
    "            \n",
    "            # Count correct predictions\n",
    "            correct += (predicted[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "            total += non_pad_mask.sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Create a DataLoader for the training data\n",
    "train_loader_for_eval = DataLoader(addition_train, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_accuracy = evaluate_on_dataset(loaded_model, train_loader_for_eval, \"Training Data\")\n",
    "\n",
    "# Evaluate on test data for comparison\n",
    "test_accuracy = evaluate_on_dataset(loaded_model, test_loader, \"Test Data\")\n",
    "\n",
    "# Print comparison\n",
    "print(f\"\\nAccuracy comparison:\")\n",
    "print(f\"Training Data: {train_accuracy:.4f}\")\n",
    "print(f\"Test Data:     {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 100 large number samples (21-30 digits): 0.0000\n",
      "Testing on specific addition patterns:\n",
      "999999 + 1 = 99991999900 (True: 1000000)\n",
      "Correct: False\n",
      "\n",
      "1 + 999999 = 11999000 (True: 1000000)\n",
      "Correct: False\n",
      "\n",
      "999999999999999 + 1 = 00999990099999990999999999999999900 (True: 1000000000000000)\n",
      "Correct: False\n",
      "\n",
      "1000000000000000 + 1000000000000000 = 0000011010002000000000000000 (True: 2000000000000000)\n",
      "Correct: False\n",
      "\n",
      "123456789 + 987654321 = 91951616571110877600 (True: 1111111110)\n",
      "Correct: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_larger_additions(model, max_seq_length, num_samples=100):\n",
    "    correct = 0\n",
    "    for _ in range(num_samples):\n",
    "        num1 = random.randint(10**20, 10**30 - 1)  # 21 to 30 digit numbers\n",
    "        num2 = random.randint(10**20, 10**30 - 1)\n",
    "        true_result = num1 + num2\n",
    "        \n",
    "        input_str = f\"{num1}+{num2}=\"\n",
    "        input_tensor = preprocess_input(input_str, max_seq_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        predicted_result = decode_output(output)\n",
    "        \n",
    "        try:\n",
    "            if int(predicted_result) == true_result:\n",
    "                correct += 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    print(f\"Accuracy on {num_samples} large number samples (21-30 digits): {accuracy:.4f}\")\n",
    "\n",
    "# Test on larger numbers\n",
    "test_larger_additions(loaded_model, checkpoint['max_seq_length'])\n",
    "\n",
    "def test_specific_patterns(model, max_seq_length):\n",
    "    test_cases = [\n",
    "        (999999, 1),  # Testing carry over\n",
    "        (1, 999999),  # Testing different order\n",
    "        (10**15 - 1, 1),  # Large number + small number\n",
    "        (10**15, 10**15),  # Two large, round numbers\n",
    "        (123456789, 987654321),  # Ascending + descending\n",
    "    ]\n",
    "    \n",
    "    for num1, num2 in test_cases:\n",
    "        input_str = f\"{num1}+{num2}=\"\n",
    "        input_tensor = preprocess_input(input_str, max_seq_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        predicted_result = decode_output(output)\n",
    "        true_result = num1 + num2\n",
    "        \n",
    "        print(f\"{num1} + {num2} = {predicted_result} (True: {true_result})\")\n",
    "        print(f\"Correct: {int(predicted_result) == true_result}\")\n",
    "        print()\n",
    "\n",
    "# Test on specific patterns\n",
    "print(\"Testing on specific addition patterns:\")\n",
    "test_specific_patterns(loaded_model, checkpoint['max_seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in SmallTransformer forward pass: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#summary(your_model, input_size=(batch_size, seq_length))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Summary of my model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Pytorch/lib/python3.9/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mSmallTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in SmallTransformer forward pass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mSmallTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# Apply Abacus Embedding\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# Pass through the Transformer encoder\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mAbacusEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(pos, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get the token embeddings\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Get the positional embeddings\u001b[39;00m\n\u001b[1;32m     25\u001b[0m positional \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(pos)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Pytorch/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Pytorch/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "#summary(your_model, input_size=(batch_size, seq_length))\n",
    "# Summary of my model\n",
    "summary(loaded_model, input_size=(32, 42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
